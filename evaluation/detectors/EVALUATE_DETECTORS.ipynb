{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from typing import List, Dict, Tuple\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detektor Evaluation\n",
    "- Finde Repeatability und Accuracy für die verschiedenen Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_nearest_neighbour_without_duplicates(\n",
    "    values:np.array) -> Tuple[np.array, np.array]:\n",
    "    \"\"\"Find Nearest Neighbour without two keypoints form image j are appointed to the same keypoint in image i.\n",
    "    1) If two keypoints kp_j1 and kp_j2 are appointed to the same keypoint kp_i1, then find the better matching one and keep it.\n",
    "    2) The other match is discarded and the next best match for that\n",
    "    keypoint is taken. \n",
    "    3) Check for duplicate matches and go back to 1) if the situation arises.\n",
    "    \"\"\"\n",
    "\n",
    "    # For each kp_j find the best matching order (ascending distance) for all\n",
    "    # of kp_i.\n",
    "    l2 = values.copy()\n",
    "    min_idx = l2.argsort(axis=1)\n",
    "\n",
    "    # Prevents infinte loop, if more kp_j exists than kp_i\n",
    "    _MAX_ITERATION_COUNT = l2.shape[1]\n",
    "    _iteration_count = 0\n",
    "\n",
    "    # Assume, duplicates exists to test at least once.\n",
    "    has_duplicates = True\n",
    "\n",
    "    while(has_duplicates and _iteration_count < _MAX_ITERATION_COUNT):\n",
    "        has_duplicates = False\n",
    "        _iteration_count += 1\n",
    "\n",
    "        # Get number of unique ids of the best matches kp_i and count, how\n",
    "        # often kp_1 has been assigned.\n",
    "        _dup_idx, _counts = np.unique(min_idx[:, 0], return_counts=True)\n",
    "        dup_and_count = np.vstack([_dup_idx,_counts]).T\n",
    "\n",
    "        # Check for actual duplicate assignments. If ids from kp_i has been \n",
    "        # assigned multiple times, the number of unique ids will be smaller\n",
    "        # than the number of kp_j (= rows). \n",
    "        if dup_and_count.shape[0] != min_idx.shape[0]:\n",
    "            has_duplicates = True\n",
    "\n",
    "            # Count how often an Keypoint kp_i has been matched more than once\n",
    "            # and get the first kp_i id. This kp_x will be fixed first.\n",
    "            gt_1 = dup_and_count[:, 1] > 1\n",
    "            dup_id = dup_and_count[gt_1][0][0]\n",
    "\n",
    "            # Find all matches where kp_x is the best match. Those rows are \n",
    "            # candidates to be fixed.\n",
    "            candidates = min_idx[:, 0] == dup_id\n",
    "\n",
    "            # Of all those candidates, find the match with the smallest value.\n",
    "            # This is the best match for that kp_x.\n",
    "            # All other rows with kp_x as best match are going to be rotated.\n",
    "            l2_args = l2[:, 0].argsort()\n",
    "            cand_args = candidates[l2_args]\n",
    "            best_candidates_id = l2[:, 0].argsort()[cand_args][0]\n",
    "\n",
    "            # Mark the best match as \"False\" to not rotate that row.\n",
    "            candidates[best_candidates_id] = False\n",
    "\n",
    "            # Then circle to all the other matches and rotate the row to get \n",
    "            # the next best match to be their best match.\n",
    "            for id_c, val_c in enumerate(candidates):\n",
    "                if val_c:\n",
    "                min_idx[id_c] = np.roll(min_idx[id_c], -1)\n",
    "                l2[id_c] = np.roll(l2[id_c], -1)\n",
    "\n",
    "        # If the while loop ends, all kp_i have been assigned once.\n",
    "        # Return the best match ids of kp_i and their respective distance values.\n",
    "        return l2[:, 0], min_idx[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/40 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start evaluation of detector sift.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [23:39<00:00, 35.50s/it]\n",
      "100%|██████████| 50/50 [05:20<00:00,  6.42s/it]\n",
      "100%|██████████| 40/40 [29:59<00:00, 44.98s/it]  \n",
      "100%|██████████| 40/40 [05:33<00:00,  8.34s/it]\n",
      "100%|██████████| 40/40 [06:42<00:00, 10.06s/it]\n",
      "100%|██████████| 40/40 [08:36<00:00, 12.92s/it]\n",
      "  0%|          | 0/40 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation of detector sift complete.\n",
      "Start evaluation of detector lift.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [05:30<00:00,  8.26s/it]\n",
      "100%|██████████| 50/50 [05:14<00:00,  6.29s/it]\n",
      "100%|██████████| 40/40 [15:18<00:00, 22.96s/it]\n",
      "100%|██████████| 40/40 [03:31<00:00,  5.30s/it]\n",
      "100%|██████████| 40/40 [05:33<00:00,  8.33s/it]\n",
      "100%|██████████| 40/40 [05:56<00:00,  8.92s/it]\n",
      "  0%|          | 0/40 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation of detector lift complete.\n",
      "Start evaluation of detector tcovdet.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [03:08<00:00,  4.71s/it]\n",
      "100%|██████████| 50/50 [02:09<00:00,  2.58s/it]\n",
      "100%|██████████| 40/40 [05:51<00:00,  8.78s/it]\n",
      "100%|██████████| 40/40 [01:31<00:00,  2.30s/it]\n",
      "100%|██████████| 40/40 [02:28<00:00,  3.71s/it]\n",
      "100%|██████████| 40/40 [02:28<00:00,  3.72s/it]\n",
      "  0%|          | 0/40 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation of detector tcovdet complete.\n",
      "Start evaluation of detector tilde.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [03:18<00:00,  4.97s/it]\n",
      "100%|██████████| 50/50 [02:08<00:00,  2.57s/it]\n",
      "100%|██████████| 40/40 [04:07<00:00,  6.19s/it]\n",
      "100%|██████████| 40/40 [01:44<00:00,  2.61s/it]\n",
      "100%|██████████| 40/40 [02:17<00:00,  3.44s/it]\n",
      "100%|██████████| 40/40 [02:24<00:00,  3.62s/it]\n",
      "  0%|          | 0/40 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation of detector tilde complete.\n",
      "Start evaluation of detector superpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [3:00:43<00:00, 271.10s/it]  \n",
      "100%|██████████| 50/50 [3:11:19<00:00, 229.58s/it]  \n",
      "100%|██████████| 40/40 [3:04:01<00:00, 276.05s/it]  \n",
      "100%|██████████| 40/40 [1:46:23<00:00, 159.59s/it]  \n",
      "100%|██████████| 40/40 [3:04:14<00:00, 276.37s/it]  \n",
      "100%|██████████| 40/40 [2:48:36<00:00, 252.91s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation of detector superpoint complete.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def get_set_names(data_dir:str, sort_output:bool=True) -> List[str]:\n",
    "    set_names = [x for x in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, x))]\n",
    "\n",
    "    if sort_output:\n",
    "        set_names = sorted(set_names)\n",
    "    \n",
    "    return set_names\n",
    "\n",
    "def get_file_names_in_set(path_set:str, file_sheme:str, sort_output:bool=True) -> List[str]:\n",
    "    file_names = [x for x in os.listdir(path_set) if os.path.isfile(os.path.join(path_set, x))]\n",
    " \n",
    "    # get the correct files with fitting file scheme.\n",
    "    file_names = [x for x in file_names if file_scheme in x]\n",
    "\n",
    "    if sort_output:\n",
    "        file_names = sorted(file_names)\n",
    "\n",
    "    return file_names\n",
    "\n",
    "def evaluate_detector(\n",
    "    detector_name:str,\n",
    "    collection_name:str,\n",
    "    path_collection:str,\n",
    "    set_names:List[str],\n",
    "    file_scheme:str,\n",
    "    keypoint_thresholds:List[int],\n",
    "    dist_error_thresholds:List[float],\n",
    "    error_vals_per_set:Dict,\n",
    "    column_names:List[str],\n",
    "    fast_eval:bool=False) -> pd.DataFrame:\n",
    "\n",
    "    # Create output dataframe.\n",
    "    df = pd.DataFrame(columns=column_names)\n",
    "\n",
    "    for set_name in set_names:\n",
    "        path_set = os.path.join(path_collection, set_name, 'keypoints', detector_name)\n",
    "        file_names = get_file_names_in_set(path_set, file_scheme)\n",
    "        num_files = 10 if fast_eval else len(file_names)\n",
    "        for i in tqdm(range(num_files)):\n",
    "            path_f1 = os.path.join(path_set, file_names[i])\n",
    "            f1 = pd.read_csv(path_f1, sep=',', header=None, usecols=[0, 1], comment='#').values.astype('float32')\n",
    "            num_kpts_i = f1.shape[0]\n",
    "\n",
    "            for j in range(i+1,num_files,1):\n",
    "                path_f2 = os.path.join(path_set, file_names[j])\n",
    "                f2 = pd.read_csv(path_f2, sep=',', header=None, usecols=[0, 1], comment='#').values.astype('float32')\n",
    "                num_kpts_j = f2.shape[0]\n",
    "\n",
    "\n",
    "                for kp_thresh in keypoint_thresholds:\n",
    "                    _f1 = f1[:kp_thresh]\n",
    "                    _f2 = f2[:kp_thresh]\n",
    "\n",
    "                    # Number of maximal possible matches for first t keypoints.\n",
    "                    max_num_matches = np.min([len(_f1), len(_f2)])\n",
    "\n",
    "                    # Each row k contains the differences f2_k - f1, for all\n",
    "                    # f1_l in f1.\n",
    "                    # [[f2_0 - f1_0, f2_0 - f1_1, ..., f2_0 - f1_l],\n",
    "                    #  [...]\n",
    "                    #  [f2_m - f1_0, f2_m - f1_1, ..., f2_m - f1_l]]\n",
    "                    _d = np.linalg.norm(_f2 - _f1[:, np.newaxis], axis=2)\n",
    "                    \n",
    "                    # Get the index of the lowest squared difference for each row\n",
    "                    sorted_idx = _d.argsort(axis=1)\n",
    "                    _nn = sorted_idx[:, 0] # indices of the nearest neighbour kpts\n",
    "\n",
    "                    # Get the corresponding d value\n",
    "                    _d = _d[:, _nn][:, 0]\n",
    "                    \n",
    "                    for dist_percentage in dist_error_thresholds:\n",
    "                        # Compute the distance threshold as L2 Norm value\n",
    "                        dist_thresh = error_vals_per_set[set_name] * dist_percentage\n",
    "                        dist_thresh = np.linalg.norm(dist_thresh)\n",
    "                        \n",
    "                        # Remove all entries, that violate dist_thresh.\n",
    "                        nn = _nn[_d <= dist_thresh]\n",
    "                        d = _d[_d <= dist_thresh]\n",
    "                        \n",
    "                        # Find duplicates\n",
    "                        _, u_idx = np.unique(nn, return_index=True)\n",
    "\n",
    "                        # Remove duplicates\n",
    "                        d = d[u_idx]\n",
    "\n",
    "                        # mean distance of all hit\n",
    "                        min_dist = np.min(d) if len(d) else -1\n",
    "                        max_dist = np.max(d) if len(d) else -1\n",
    "                        mean_dist = np.mean(d) if len(d) else -1\n",
    "                        std_dist = np.std(d) if len(d) else -1\n",
    "\n",
    "                        # repeatability: ratio of matches and number of maximal possible\n",
    "                        # matches:\n",
    "                        num_matches = len(d)\n",
    "                        repeatability = 0 if max_num_matches == 0 else num_matches / max_num_matches\n",
    "                        \n",
    "                        # The accuracy of a match is 1 - (l2.distance / dist_tresh)\n",
    "                        # And the accuracy for the image pair is the mean of all accuracie values\n",
    "                        #acc_values = 1.0 - (d / dist_thresh)\n",
    "                        #acc_values = 1.0 - np.divide(d, dist_thresh, out=np.zeros_like(d), where=d > 0)\n",
    "                        acc_values = np.divide(1.0, np.sqrt(d + 1.0), out=np.zeros_like(d), where=d>=0)\n",
    "                        accuracy = np.mean(acc_values)\n",
    "                        \n",
    "                        # Append new row to dataframe.\n",
    "                        df = df.append({\n",
    "                            'collection_name': collection_name,\n",
    "                            'set_name': set_name,\n",
    "                            'detector_name': detector_name,\n",
    "                            'image_i': file_names[i],\n",
    "                            'image_j': file_names[j],\n",
    "                            'num_kpts_i': num_kpts_i,\n",
    "                            'num_kpts_j': num_kpts_j,\n",
    "                            'keypoint_threshold': kp_thresh,\n",
    "                            'dist_threshold': dist_percentage,\n",
    "                            'max_num_matches': max_num_matches,\n",
    "                            'num_matches': num_matches,\n",
    "                            'mean_dist': mean_dist,\n",
    "                            'std_dist': std_dist,\n",
    "                            'min_dist': min_dist,\n",
    "                            'max_dist': max_dist,\n",
    "                            'min_dist': min_dist,\n",
    "                            'max_dist': max_dist,\n",
    "                            'repeatability': repeatability,\n",
    "                            'accuracy': accuracy\n",
    "                            }, ignore_index=True)\n",
    "    return df\n",
    "\n",
    "def save_output_for_detector(\n",
    "    path_output:str,\n",
    "    detector_name:str,\n",
    "    collection_name:str,\n",
    "    df:pd.DataFrame) -> None:\n",
    "\n",
    "    fout_name = '{}_{}.csv'.format(detector_name, collection_name)\n",
    "    if not os.path.exists(path_output):\n",
    "        os.makedirs(path_output, exist_ok=True)\n",
    "\n",
    "    df.to_csv(os.path.join(path_output, fout_name), \n",
    "              index=False, \n",
    "              encoding='utf-8')\n",
    "\n",
    "\n",
    "#################################\n",
    "### PARAMS\n",
    "#################################\n",
    "\n",
    "root_dir = '/home/mizzade/Workspace/diplom/code' # Adjust this accordingly\n",
    "data_dir = 'outputs'\n",
    "output_dir = 'output_evaluation'\n",
    "path_output = os.path.join(root_dir, output_dir, 'detectors')\n",
    "\n",
    "collection_name = 'webcam'\n",
    "path_collection = os.path.join(root_dir, data_dir, collection_name)\n",
    "\n",
    "file_scheme = '_10000.csv'\n",
    "detector_names = ['sift', 'lift', 'tcovdet' , 'tilde', 'superpoint']\n",
    "detector_name = 'sift'\n",
    "\n",
    "set_names = get_set_names(path_collection, sort_output=True)\n",
    "keypoint_thresholds = [1000, 5000, 10000]\n",
    "#dist_error_thresholds = [1, 5, 10, 15, 20, 25, 30, 35, 40]\n",
    "dist_error_thresholds = [14]\n",
    "#dist_error_thresholds = np.linspace(1,200, 200)\n",
    "fast_eval = False\n",
    "\n",
    "\n",
    "# Width, Height\n",
    "# Number of pixels for width and height that make 1% of the corresponding\n",
    "# dimension for the images in the corresponding set.\n",
    "error_vals_per_set = {\n",
    "    'chamonix': np.array([704, 547]) / 100.0,\n",
    "    'courbevoie': np.array([640, 471]) / 100.0,\n",
    "    'frankfurt': np.array([1024, 627]) / 100.0,\n",
    "    'mexico': np.array([640, 418]) / 100.0,\n",
    "    'panorama': np.array([2469, 205]) / 100.0,\n",
    "    'stlouis': np.array([800, 450]) / 100.0,\n",
    "    'v_xxl': np.array([1000, 1000]) / 100.0\n",
    "}\n",
    "\n",
    "# 'collection_name':str           Name of the collection.\n",
    "# 'set_name':str                  Name of the set.         \n",
    "# 'detector_name':str             Name of the detector.\n",
    "# 'image_i':str                   Name of the first (left) image.\n",
    "# 'image_j':str                   Name of the second (right) image.\n",
    "# 'num_kpts_i':int                Number of keypoints found in first image.\n",
    "# 'num_kpts_j':int                Number of keypoints found in the second image.\n",
    "# 'keypoint_threshold':int        Number of keypoints to use. [1000, 5000, 10000].\n",
    "# 'dist_percentage':float         Maximal match distance in percentage to count \n",
    "#                                 as match.Relative to image dimensions. [1, 5, 10]\n",
    "# 'max_num_matches':int           Maximal number of possible matches under current \n",
    "#                                 conditions.\n",
    "# 'num_matches':int               Actual number of matches.\n",
    "# 'mean_dist':float               Average L2 distance between two matches.\n",
    "# 'std_dist':float                Standard deviation of the l2 distance between \n",
    "#                                 two matches.\n",
    "# 'min_dist':float                Smallest L2 distance between two matches.\n",
    "# 'max_dist':float                Largest L2 distance between two matches.\n",
    "# 'repeatability':float\n",
    "column_names = ['collection_name','set_name', 'detector_name', \n",
    "                'image_i', 'image_j', 'num_kpts_i', 'num_kpts_j', \n",
    "                'keypoint_threshold', 'dist_threshold', \n",
    "                'max_num_matches', 'num_matches', 'mean_dist', \n",
    "                'std_dist', 'min_dist', 'max_dist', 'repeatability', 'accuracy']\n",
    "\n",
    "# Test\n",
    "# detector_names = ['sift']\n",
    "# collection_name = 'example'\n",
    "# set_names = ['v_xxl']\n",
    "# path_collection = os.path.join(root_dir, data_dir, collection_name)\n",
    "# error_vals_per_set['v_xxl'] = np.array([1000, 1000]) / 100.0\n",
    "\n",
    "#################################\n",
    "### MAIN\n",
    "#################################\n",
    "\n",
    "for detector_name in detector_names:\n",
    "    print('Start evaluation of detector {}.'.format(detector_name))\n",
    "    df = evaluate_detector(\n",
    "     detector_name,\n",
    "     collection_name,\n",
    "     path_collection,\n",
    "     set_names,\n",
    "     file_scheme,\n",
    "     keypoint_thresholds,\n",
    "     dist_error_thresholds,\n",
    "     error_vals_per_set,\n",
    "     column_names,\n",
    "     fast_eval=fast_eval)\n",
    "\n",
    "    save_output_for_detector(\n",
    "        path_output, \n",
    "        detector_name, \n",
    "        collection_name, \n",
    "        df)\n",
    "\n",
    "    print('Evaluation of detector {} complete.'.format(detector_name))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": false,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
